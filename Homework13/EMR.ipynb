{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e72cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyspark==3.5.1 pandas pyarrow numpy jupyter ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cf5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Homework13\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ae1068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of pi is estimated to be 3.134000\n"
     ]
    }
   ],
   "source": [
    "# this code is to estimate the value of pi\n",
    "# we will use the Monte Carlo method to estimate the value of pi\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "n = 1000000 #number of points\n",
    "def inside(partition):\n",
    "    count = 0\n",
    "    # randomly generates a point in the unit square (between 0 and 1)\n",
    "    for _ in partition:\n",
    "        x, y = random(), random()\n",
    "        if x*x + y*y < 1:\n",
    "            count += 1\n",
    "    yield count\n",
    "\n",
    "# we will create a parallelized RDD to count the number of points inside the unit circle\n",
    "# if it is inside the unit circle, we will return 1, otherwise 0\n",
    "count = sc.parallelize(range(n), 500).mapPartitions(inside).reduce(add)\n",
    "\n",
    "# then we will calculate the value of pi\n",
    "pi = 4 * count / n\n",
    "print('The value of pi is estimated to be %f' % pi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956ede07-edbc-43c6-b0a8-d2cd8c852485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, np.int32(13)), (0, 1, np.int32(8)), (0, 2, np.int32(19)), (0, 3, np.int32(12)), (0, 4, np.int32(4)), (0, 5, np.int32(18)), (1, 0, np.int32(14)), (1, 1, np.int32(10)), (1, 2, np.int32(10)), (1, 3, np.int32(18)), (1, 4, np.int32(4)), (1, 5, np.int32(16)), (2, 0, np.int32(12)), (2, 1, np.int32(1)), (2, 2, np.int32(18)), (2, 3, np.int32(7)), (2, 4, np.int32(13)), (2, 5, np.int32(15))]\n",
      "[(0, 0, np.int32(7)), (0, 1, np.int32(6)), (0, 2, np.int32(9)), (0, 3, np.int32(19)), (1, 0, np.int32(5)), (1, 1, np.int32(1)), (1, 2, np.int32(16)), (1, 3, np.int32(11)), (2, 0, np.int32(17)), (2, 1, np.int32(8)), (2, 2, np.int32(5)), (2, 3, np.int32(6)), (3, 0, np.int32(15)), (3, 1, np.int32(4)), (3, 2, np.int32(14)), (3, 3, np.int32(8)), (4, 0, np.int32(15)), (4, 1, np.int32(17)), (4, 2, np.int32(7)), (4, 3, np.int32(2)), (5, 0, np.int32(10)), (5, 1, np.int32(18)), (5, 2, np.int32(3)), (5, 3, np.int32(9))]\n",
      "[((0, 0), np.int32(874)), ((0, 1), np.int32(678)), ((0, 2), np.int32(590)), ((0, 3), np.int32(715)), ((1, 0), np.int32(808)), ((1, 1), np.int32(602)), ((1, 2), np.int32(664)), ((1, 3), np.int32(732)), ((2, 0), np.int32(845)), ((2, 1), np.int32(736)), ((2, 2), np.int32(448)), ((2, 3), np.int32(564))]\n",
      "(0, 0) = 874\n",
      "(0, 1) = 678\n",
      "(0, 2) = 590\n",
      "(0, 3) = 715\n",
      "(1, 0) = 808\n",
      "(1, 1) = 602\n",
      "(1, 2) = 664\n",
      "(1, 3) = 732\n",
      "(2, 0) = 845\n",
      "(2, 1) = 736\n",
      "(2, 2) = 448\n",
      "(2, 3) = 564\n"
     ]
    }
   ],
   "source": [
    "# this code is to generate matrices to perform matrix multiplication\n",
    "import numpy as np\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# use numpy to generate two matrices\n",
    "# make assumption of the matrix size\n",
    "m, n, p = 3, 6, 4\n",
    "matrix_a = np.random.randint(1, 20, (m, n))\n",
    "matrix_b = np.random.randint(1, 20, (n, p))\n",
    "\n",
    "# convert the matrices to lists\n",
    "A = [(i, k, matrix_a[i, k]) for i in range(m) for k in range(n)]\n",
    "B = [(k, j, matrix_b[k, j]) for k in range(n) for j in range(p)]\n",
    "\n",
    "# convert A and B to RDDs\n",
    "rddA = sc.parallelize(A)\n",
    "rddB = sc.parallelize(B)\n",
    "\n",
    "print(rddA.collect())\n",
    "print(rddB.collect())\n",
    "\n",
    "# perform matrix multiplication\n",
    "# in this step, we will first map the elements of A and B \n",
    "# as for A, we will map the element to (k, (i, matrix_a[i, k]))\n",
    "# as for B, we will map the element to (k, (j, matrix_b[k, j]))\n",
    "# then we will join the two RDDs and do the multiplication\n",
    "# we will use the reduceByKey to sum the results\n",
    "# after that, we can get the matrix multiplication result\n",
    "rddA2 = rddA.map(lambda x: (x[1], (x[0], x[2])))\n",
    "rddB2 = rddB.map(lambda x: (x[0], (x[1], x[2])))\n",
    "\n",
    "def multiply(x):\n",
    "    k, ((i, a_ik), (j, b_kj)) = x\n",
    "    return ((i, j), a_ik * b_kj)\n",
    "rddC = rddA2.join(rddB2).map(multiply)\n",
    "rddC_result = rddC.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(rddC_result.sortByKey().collect())\n",
    "for (i, j), value in rddC_result.sortByKey().collect():\n",
    "    print(f\"({i}, {j}) = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ae8396-7c78-48ff-8a8b-b6185459c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'salmon', 'rat']\n",
      "[]\n",
      "['dog']\n",
      "['dog', 'salmon']\n",
      "[(3, 'dog'), (6, 'salmon'), (3, 'rat')]\n"
     ]
    }
   ],
   "source": [
    "# this code is to convert the code from scala to pyspark\n",
    "# we will use the code from the class to do the conversion\n",
    "# this is the code from RDD Transformations 1\n",
    "rdd1 = sc.parallelize([\"dog\", \"salmon\", \"rat\"], 2)\n",
    "rdd2 = rdd1.map(lambda x: (len(x), x))\n",
    "print(rdd1.collect())\n",
    "print(rdd1.take(0))\n",
    "print(rdd1.take(1))\n",
    "print(rdd1.take(2))\n",
    "print(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50e349f-a671-454f-b54d-9a741ba6b9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[[1, 1, 1], [2, 2, 2], [3, 3, 3]]\n",
      "[1, 1, 1, 2, 2, 2, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# this code is to convert the code from scala to pyspark\n",
    "# we will use the code from the class to do the conversion\n",
    "# this is the code from RDD Transformations 2\n",
    "rdd1 = sc.parallelize([1, 2, 3], 2)\n",
    "rdd2 = rdd1.map(lambda x: [x, x, x])\n",
    "rdd3 = rdd1.flatMap(lambda x: [x, x, x])\n",
    "\n",
    "print(rdd1.collect())\n",
    "print(rdd2.collect())\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffff71b6-702d-44fd-96c0-dced1abe51a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# this code is to convert the code from scala to pyspark\n",
    "# we will use the code from the class to do the conversion\n",
    "# this is the code from RDD Transformations 3\n",
    "rdd1 = sc.parallelize(range(1,11), 1)\n",
    "rdd2 = rdd1.filter(lambda x: (x % 2 == 0))\n",
    "\n",
    "print(rdd1.collect())\n",
    "print(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45dfa6e-0593-4f19-a4bc-8b856dc5317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'dog'), (6, 'salmon'), (3, 'rat')]\n",
      "[(3, 'cat'), (6, 'rabbit'), (5, 'horse')]\n",
      "[(6, ('salmon', 'rabbit')), (3, ('dog', 'cat')), (3, ('rat', 'cat'))]\n"
     ]
    }
   ],
   "source": [
    "# this code is to convert the code from scala to pyspark\n",
    "# we will use the code from the class to do the conversion\n",
    "# this is the code from RDD Transformations 4\n",
    "rdd1 = sc.parallelize([\"dog\", \"salmon\", \"rat\"], 2)\n",
    "rdd2 = sc.parallelize([\"cat\", \"rabbit\", \"horse\"], 2)\n",
    "rdd3 = rdd1.map(lambda x: (len(x), x))\n",
    "rdd4 = rdd2.map(lambda x: (len(x), x))\n",
    "rdd5 = rdd3.join(rdd4)\n",
    "\n",
    "print(rdd3.collect())\n",
    "print(rdd4.collect())\n",
    "print(rdd5.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed80b95-37d4-4b4f-9e64-f1866d59ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['salmon']\n",
      "3 ['dog', 'rat']\n"
     ]
    }
   ],
   "source": [
    "# this code is to convert the code from scala to pyspark\n",
    "# we will use the code from the class to do the conversion\n",
    "# this is the code from RDD Transformations 5\n",
    "rdd1 = sc.parallelize([\"dog\", \"salmon\", \"rat\"], 2)\n",
    "rdd2 = rdd1.map(lambda x: (len(x), x))\n",
    "rdd3 = rdd2.groupByKey()\n",
    "\n",
    "for key, value in rdd3.collect():\n",
    "    print(key, list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97e36dba-02f6-4c12-8f72-d0c9c270be71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'dog'), (6, 'salmon'), (3, 'rat')]\n",
      "[(6, 'salmon'), (3, 'dog#rat')]\n"
     ]
    }
   ],
   "source": [
    "# this code is to convert the code from scala to pyspark\n",
    "# we will use the code from the class to do the conversion\n",
    "# this is the code from RDD Transformations 6\n",
    "rdd1 = sc.parallelize([\"dog\", \"salmon\", \"rat\"], 2)\n",
    "rdd2 = rdd1.map(lambda x: (len(x), x))\n",
    "rdd3 = rdd2.reduceByKey(lambda x, y: x + '#' + y)\n",
    "\n",
    "print(rdd2.collect())\n",
    "print(rdd3.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bfcc721-5727-491f-8ab7-e6c15a6bca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'United States of America'), ('2', 'United Kingdom of Great Britain and Northern Ireland'), ('3', 'United Kingdom of Great Britain and Northern Ireland'), ('4', 'Canada'), ('5', 'Norway'), ('6', 'United States of America'), ('7', 'United States of America'), ('8', 'Uzbekistan'), ('9', 'United Kingdom of Great Britain and Northern Ireland'), ('10', 'Serbia')]\n",
      "[('1', ('United States of America', 'Canada')), ('2', ('United Kingdom of Great Britain and Northern Ireland', 'United Kingdom of Great Britain and Northern Ireland')), ('3', ('United Kingdom of Great Britain and Northern Ireland', 'China')), ('4', ('Canada', 'Egypt')), ('5', ('Norway', 'Spain')), ('6', ('United States of America', 'United States of America')), ('7', ('United States of America', 'Germany')), ('8', ('Uzbekistan', 'Brazil'))]\n",
      "time_duration: 51.844245195388794\n",
      "[('1', ('United States of America', 'Canada')), ('2', ('United Kingdom of Great Britain and Northern Ireland', 'United Kingdom of Great Britain and Northern Ireland')), ('3', ('United Kingdom of Great Britain and Northern Ireland', 'China')), ('4', ('Canada', 'Egypt')), ('5', ('Norway', 'Spain')), ('6', ('United States of America', 'United States of America')), ('7', ('United States of America', 'Germany')), ('8', ('Uzbekistan', 'Brazil'))]\n",
      "time_duration: 8.590973854064941\n"
     ]
    }
   ],
   "source": [
    "# this code is to join two RDDS, one is large and one is much smaller\n",
    "# the large RDD used is StackOverflow RDD\n",
    "# first, read the StackOverflow RDD in S3 bucket\n",
    "from csv import reader\n",
    "import time\n",
    "\n",
    "# read the StackOverflow RDD\n",
    "file_path = \"H:/DE/DE Lecture/survey_results_public.csv\"\n",
    "stack_overflow = spark.read.csv(file_path, header = True)\n",
    "\n",
    "# get the Respondent and Country column\n",
    "filtered_stackoverflow = stack_overflow.select(\"ResponseId\", \"Country\")\n",
    "stackoverflow_rdd = filtered_stackoverflow.rdd.map(lambda row: (row.ResponseId, row.Country))\n",
    "\n",
    "# print the first 10 elements of the StackOverflow RDD to check if it is correctly read\n",
    "print(stackoverflow_rdd.take(10))\n",
    "\n",
    "# create a much smaller RDD\n",
    "rdd1 = sc.parallelize([('1', 'Canada'), ('2', 'United Kingdom of Great Britain and Northern Ireland'), ('3', 'China'), \n",
    "('4', 'Egypt'), ('5', 'Spain'), ('6', 'United States of America'), ('7', 'Germany'), ('8', 'Brazil')])\n",
    "\n",
    "# join the two RDDs using traditional join method\n",
    "rdd_join = stackoverflow_rdd.join(rdd1)\n",
    "start_time = time.time()\n",
    "count = rdd_join.count()\n",
    "time_duration = time.time() - start_time\n",
    "\n",
    "print(rdd_join.sortByKey().take(10))\n",
    "print(f\"time_duration: {time_duration}\")\n",
    "\n",
    "# join the two RDDs using broadcast join method\n",
    "# broadcast the small RDD to all the nodes\n",
    "broadcast_rdd1 = sc.broadcast(dict(rdd1.collect()))\n",
    "# get the broadcasted RDD from the executor\n",
    "# use map join method to join the two RDDs\n",
    "start_time = time.time()\n",
    "# use map join method to join the two RDDs\n",
    "def map(record):\n",
    "    ResponseId, Country = record\n",
    "    if ResponseId in broadcast_rdd1.value:\n",
    "        return (ResponseId, (Country, broadcast_rdd1.value[ResponseId]))\n",
    "    else:\n",
    "        return (ResponseId, None)\n",
    "\n",
    "rdd_broadcast_join = stackoverflow_rdd.map(map).filter(lambda x: x[1] is not None)\n",
    "count = rdd_broadcast_join.count()\n",
    "time_duration = time.time() - start_time\n",
    "\n",
    "print(rdd_broadcast_join.take(10))\n",
    "print(f\"time_duration: {time_duration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f77ce53-c27e-4ca7-9dc4-8478e91afcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'United States of America'), ('2', 'United Kingdom of Great Britain and Northern Ireland'), ('3', 'United Kingdom of Great Britain and Northern Ireland'), ('4', 'Canada'), ('5', 'Norway'), ('6', 'United States of America'), ('7', 'United States of America'), ('8', 'Uzbekistan'), ('9', 'United Kingdom of Great Britain and Northern Ireland'), ('10', 'Serbia')]\n",
      "The number of Singapore respondents is 177\n",
      "The number of Singapore respondents is 177\n"
     ]
    }
   ],
   "source": [
    "# this code is to aggregate speficic metrics from the StackOverflow RDD\n",
    "# we will aggregate the country metrics\n",
    "# read the StackOverflow RDD\n",
    "file_path = \"H:/DE/DE Lecture/survey_results_public.csv\"\n",
    "stack_overflow = spark.read.csv(file_path, header = True)\n",
    "\n",
    "# get the Respondent and Country column\n",
    "filtered_stackoverflow = stack_overflow.select(\"ResponseId\", \"Country\")\n",
    "stackoverflow_rdd = filtered_stackoverflow.rdd.map(lambda row: (row.ResponseId, row.Country))\n",
    "\n",
    "# print the first 10 elements of the StackOverflow RDD to check if it is correctly read\n",
    "print(stackoverflow_rdd.take(10))\n",
    "\n",
    "# use traditional method to aggregate the country metrics\n",
    "count = stackoverflow_rdd.filter(lambda x: x[1] == \"Singapore\").count()\n",
    "print(f\"The number of Singapore respondents is {count}\")\n",
    "\n",
    "# use the accumulator to aggregate the country metrics\n",
    "accumulator = sc.accumulator(0)\n",
    "stackoverflow_rdd.foreach(lambda x: accumulator.add(1) if x[1] == \"Singapore\" else None)\n",
    "print(f\"The number of Singapore respondents is {accumulator.value}\")\n",
    "\n",
    "# Based on the result, it can be concluded that the accumulator method matches the traditional method\n",
    "# It means that the accumulator method can also generate the expected result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b49aa-fef0-4ca4-a968-cbd687913ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
